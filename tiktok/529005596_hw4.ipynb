{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUStugfzWud2"
      },
      "source": [
        "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2022\n",
        "\n",
        "\n",
        "# Homework 4: The Final One!\n",
        "\n",
        "- **100 points [7% of your final grade]**\n",
        "- **Due April 28, 2022 11:59pm** \n",
        "- (*no submissions accepted after May 3, 2022 by 11:59pm*)\n",
        "\n",
        "*Goals of this homework:* This is an **open-ended** assignment in which you will get to explore embeddings in the context of a large social media dataset. \n",
        "\n",
        "*Submission instructions:* You should post your notebook to canvas (look for the homework 4 assignment there). Name your submission **your-uin_hw4.ipynb**, so for example, my submission would be something like **555001234_hw4.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. When you are done, download your notebook from colab and submit it to canvas.\n",
        "\n",
        "*Collaboration declaration:* If you talked to anyone about this homework, please be sure to mention that. Remember to include citations to any sources you use in the homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eql4ThhZt5lL"
      },
      "source": [
        "*Write your collaboration/references here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AQt_pCoWueB"
      },
      "source": [
        "## Instructions: \n",
        "\n",
        "For this homework, you will conduct an independent data mining analysis using embeddings of the congressional Twitter dataset we used in Homework 2. The requirements are threefold:\n",
        "\n",
        "*   You must use some (or all) of the US Congress Twitter data we provided in Homework 2. You may augment the data if you like, but that is not required. Also feel free to sample just a portion of the data (you do not have to use all of it).\n",
        "*   You must learn embeddings over the dataset. You may use word2vec to find word embeddings, node2vec to find node embeddings, or some other dense representation. It is up to you, but you must learn some kind of embedding. You may use an existing package to learn these embeddings. Feel free to google around to find the right resource. Some examples include: \n",
        "\n",
        "  * word2vec (python, in Gensim) https://radimrehurek.com/gensim/models/word2vec.html\n",
        "  * doc2vec (python, in Gensim) https://radimrehurek.com/gensim/models/doc2vec.html\n",
        "  * Node2Vec (python) https://github.com/aditya-grover/node2vec\n",
        "  * LINE (C++) https://github.com/tangjianpku/LINE\n",
        "  * DeepWalk (python) https://github.com/phanein/deepwalk, https://pypi.org/project/deepwalk/\n",
        "\n",
        "* Using these embeddings, you must conduct an interesting analysis of the dataset. For example, are there communities of users you can discover in the node embedding space? Are there outliers in the word embedding space? This is the most interesting part, and you should do your best to make this compelling.\n",
        "\n",
        "\n",
        "### In the following, you should answer the questions we pose. Then below, you should include your code, including helpful comments and discussion so we can follow your logic.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2vrLOmWueh"
      },
      "source": [
        "##  Question 1\n",
        "\n",
        "What is your compelling data mining application? That is, what is it you aim to discover?\n",
        "\n",
        "*your answer here*\n",
        "\n",
        "Use graph node embeddings to predict whether two nodes are exactly connected or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spS3kPcxdca-"
      },
      "source": [
        "##  Question 2\n",
        "\n",
        "What data did you use? Describe briefly what aspects of the US Congressional data you used.\n",
        "\n",
        "*your answer here*\n",
        "\n",
        "I use the congress_members.csv to find all congress members' twitter id as graph nodes, and user_mentions.csv to find the mention relationship between congress members for graph edges. I use 80% of edges for training and 20% of edges for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NzuIWbDdc4v"
      },
      "source": [
        "##  Question 3\n",
        "\n",
        "What embedding method did you use? And why?\n",
        "\n",
        "*your answer here*\n",
        "\n",
        "\n",
        "I use Node2Vec embedding method. It is relatively simple and fast to run on my dataset, and has good graph structure learning abilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpK3_XVxddKE"
      },
      "source": [
        "##  Question 4\n",
        "\n",
        "How did you apply your embeddings to tackle the problem you posed?\n",
        "\n",
        "*your answer here*\n",
        "\n",
        "I first run Node2Vec on the graph with only edges in the training data. After obtaining node embeddings, for each edge in the testing data, I use the cosine similarity of two nodes connected by that edge as the probability of whether these two nodes are connected. If the similarity is larger than 0.5, the prediction is yes, i.e., predicting that those two nodes are connected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8IOv0kAddT7"
      },
      "source": [
        "##  Question 5\n",
        "\n",
        "What did you discover? Provide your results and analysis here.\n",
        "\n",
        "*your answer here*\n",
        "\n",
        "I found that the prediction accuracy is less than 0.1, which is very low. I think maybe this is because the number of training samples is small, making it hard for node embeddings to infer all exact neighbors of a node during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Analysis"
      ],
      "metadata": {
        "id": "ZiPA3pDgdw4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you should add as many cells as you need below \n",
        "!mkdir us-congress-tweets\n",
        "!wget https://us-congress.s3.amazonaws.com/congress_members.csv -O us-congress-tweets/congress_members.csv\n",
        "!wget https://us-congress.s3.amazonaws.com/user_mentions.csv -O us-congress-tweets/user_mentions.csv"
      ],
      "metadata": {
        "id": "hDUEbVs9d0Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c9a01e-505b-4a03-a379-d22db59fe888"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-21 01:10:21--  https://us-congress.s3.amazonaws.com/congress_members.csv\n",
            "Resolving us-congress.s3.amazonaws.com (us-congress.s3.amazonaws.com)... 54.231.204.185\n",
            "Connecting to us-congress.s3.amazonaws.com (us-congress.s3.amazonaws.com)|54.231.204.185|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13835 (14K) [text/csv]\n",
            "Saving to: ‘us-congress-tweets/congress_members.csv’\n",
            "\n",
            "us-congress-tweets/ 100%[===================>]  13.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-21 01:10:22 (79.0 MB/s) - ‘us-congress-tweets/congress_members.csv’ saved [13835/13835]\n",
            "\n",
            "--2022-04-21 01:10:22--  https://us-congress.s3.amazonaws.com/user_mentions.csv\n",
            "Resolving us-congress.s3.amazonaws.com (us-congress.s3.amazonaws.com)... 52.217.194.169\n",
            "Connecting to us-congress.s3.amazonaws.com (us-congress.s3.amazonaws.com)|52.217.194.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56151953 (54M) [text/csv]\n",
            "Saving to: ‘us-congress-tweets/user_mentions.csv’\n",
            "\n",
            "us-congress-tweets/ 100%[===================>]  53.55M  33.5MB/s    in 1.6s    \n",
            "\n",
            "2022-04-21 01:10:24 (33.5 MB/s) - ‘us-congress-tweets/user_mentions.csv’ saved [56151953/56151953]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "congress_ids = set()\n",
        "with open('us-congress-tweets/congress_members.csv', mode='r') as csv_file:\n",
        "  csv_reader = csv.DictReader(csv_file)\n",
        "  for row in csv_reader:\n",
        "    id = row['userid']\n",
        "    congress_ids.add(id)\n",
        "\n",
        "all_edges = []\n",
        "with open('us-congress-tweets/user_mentions.csv', mode='r') as csv_file:\n",
        "  csv_reader = csv.DictReader(csv_file)\n",
        "  for row in csv_reader:\n",
        "    src, dst = row['src'], row['dst']\n",
        "    if src in congress_ids and dst in congress_ids:\n",
        "      all_edges.append((src, dst, {'weight': 1.0}))\n",
        "\n",
        "all_nodes = set()\n",
        "for edge in all_edges:\n",
        "  src, dst, _ = edge\n",
        "  all_nodes.add(src)\n",
        "  all_nodes.add(dst)\n",
        "\n",
        "print(\"The total number of nodes is {}, the total number of edges is {}\".format(len(all_nodes), len(all_edges)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3xhHLwMjvrj",
        "outputId": "24be29d4-5f76-414b-c015-2a40045d1477"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of nodes is 254, the total number of edges is 201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "nx_G = nx.Graph()\n",
        "random.shuffle(all_edges)\n",
        "split_bound = int(0.8 * len(all_edges))\n",
        "train_edges, test_edges = all_edges[:split_bound], all_edges[split_bound:]\n",
        "nx_G.add_nodes_from(all_nodes)\n",
        "nx_G.add_edges_from(train_edges)"
      ],
      "metadata": {
        "id": "jlM0W3sOoWOz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Graph():\n",
        "\tdef __init__(self, nx_G, is_directed, p, q):\n",
        "\t\tself.G = nx_G\n",
        "\t\tself.is_directed = is_directed\n",
        "\t\tself.p = p\n",
        "\t\tself.q = q\n",
        "\n",
        "\tdef node2vec_walk(self, walk_length, start_node):\n",
        "\t\t'''\n",
        "\t\tSimulate a random walk starting from start node.\n",
        "\t\t'''\n",
        "\t\tG = self.G\n",
        "\t\talias_nodes = self.alias_nodes\n",
        "\t\talias_edges = self.alias_edges\n",
        "\n",
        "\t\twalk = [start_node]\n",
        "\n",
        "\t\twhile len(walk) < walk_length:\n",
        "\t\t\tcur = walk[-1]\n",
        "\t\t\tcur_nbrs = sorted(G.neighbors(cur))\n",
        "\t\t\tif len(cur_nbrs) > 0:\n",
        "\t\t\t\tif len(walk) == 1:\n",
        "\t\t\t\t\twalk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tprev = walk[-2]\n",
        "\t\t\t\t\tnext = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
        "\t\t\t\t\t\talias_edges[(prev, cur)][1])]\n",
        "\t\t\t\t\twalk.append(next)\n",
        "\t\t\telse:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\treturn walk\n",
        "\n",
        "\tdef simulate_walks(self, num_walks, walk_length):\n",
        "\t\t'''\n",
        "\t\tRepeatedly simulate random walks from each node.\n",
        "\t\t'''\n",
        "\t\tG = self.G\n",
        "\t\twalks = []\n",
        "\t\tnodes = list(G.nodes())\n",
        "\t\tprint('Walk iteration:')\n",
        "\t\tfor walk_iter in range(num_walks):\n",
        "\t\t\tprint('{}/{}'.format(walk_iter+1, num_walks))\n",
        "\t\t\trandom.shuffle(nodes)\n",
        "\t\t\tfor node in nodes:\n",
        "\t\t\t\twalks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
        "\n",
        "\t\treturn walks\n",
        "\n",
        "\tdef get_alias_edge(self, src, dst):\n",
        "\t\t'''\n",
        "\t\tGet the alias edge setup lists for a given edge.\n",
        "\t\t'''\n",
        "\t\tG = self.G\n",
        "\t\tp = self.p\n",
        "\t\tq = self.q\n",
        "\n",
        "\t\tunnormalized_probs = []\n",
        "\t\tfor dst_nbr in sorted(G.neighbors(dst)):\n",
        "\t\t\tif dst_nbr == src:\n",
        "\t\t\t\tunnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
        "\t\t\telif G.has_edge(dst_nbr, src):\n",
        "\t\t\t\tunnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
        "\t\t\telse:\n",
        "\t\t\t\tunnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
        "\t\tnorm_const = sum(unnormalized_probs)\n",
        "\t\tnormalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
        "\n",
        "\t\treturn alias_setup(normalized_probs)\n",
        "\n",
        "\tdef preprocess_transition_probs(self):\n",
        "\t\t'''\n",
        "\t\tPreprocessing of transition probabilities for guiding the random walks.\n",
        "\t\t'''\n",
        "\t\tG = self.G\n",
        "\t\tis_directed = self.is_directed\n",
        "\n",
        "\t\talias_nodes = {}\n",
        "\t\tfor node in G.nodes():\n",
        "\t\t\tunnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
        "\t\t\tnorm_const = sum(unnormalized_probs)\n",
        "\t\t\tnormalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
        "\t\t\talias_nodes[node] = alias_setup(normalized_probs)\n",
        "\n",
        "\t\talias_edges = {}\n",
        "\t\ttriads = {}\n",
        "\n",
        "\t\tif is_directed:\n",
        "\t\t\tfor edge in G.edges():\n",
        "\t\t\t\talias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
        "\t\telse:\n",
        "\t\t\tfor edge in G.edges():\n",
        "\t\t\t\talias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
        "\t\t\t\talias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
        "\n",
        "\t\tself.alias_nodes = alias_nodes\n",
        "\t\tself.alias_edges = alias_edges\n",
        "\n",
        "\t\treturn\n",
        "\n",
        "\n",
        "def alias_setup(probs):\n",
        "\t'''\n",
        "\tCompute utility lists for non-uniform sampling from discrete distributions.\n",
        "\tRefer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
        "\tfor details\n",
        "\t'''\n",
        "\tK = len(probs)\n",
        "\tq = np.zeros(K)\n",
        "\tJ = np.zeros(K, dtype=np.int)\n",
        "\n",
        "\tsmaller = []\n",
        "\tlarger = []\n",
        "\tfor kk, prob in enumerate(probs):\n",
        "\t    q[kk] = K*prob\n",
        "\t    if q[kk] < 1.0:\n",
        "\t        smaller.append(kk)\n",
        "\t    else:\n",
        "\t        larger.append(kk)\n",
        "\n",
        "\twhile len(smaller) > 0 and len(larger) > 0:\n",
        "\t    small = smaller.pop()\n",
        "\t    large = larger.pop()\n",
        "\n",
        "\t    J[small] = large\n",
        "\t    q[large] = q[large] + q[small] - 1.0\n",
        "\t    if q[large] < 1.0:\n",
        "\t        smaller.append(large)\n",
        "\t    else:\n",
        "\t        larger.append(large)\n",
        "\n",
        "\treturn J, q\n",
        "\n",
        "def alias_draw(J, q):\n",
        "\t'''\n",
        "\tDraw sample from a non-uniform discrete distribution using alias sampling.\n",
        "\t'''\n",
        "\tK = len(J)\n",
        "\n",
        "\tkk = int(np.floor(np.random.rand()*K))\n",
        "\tif np.random.rand() < q[kk]:\n",
        "\t    return kk\n",
        "\telse:\n",
        "\t    return J[kk]"
      ],
      "metadata": {
        "id": "U1yrPGazvKeT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = Graph(nx_G, False, 1.0, 1.0)\n",
        "G.preprocess_transition_probs()\n",
        "walks = G.simulate_walks(100, 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j3QhG-8vzyH",
        "outputId": "2050f276-5771-4ca6-f5bf-63e43a919eac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Walk iteration:\n",
            "1/100\n",
            "2/100\n",
            "3/100\n",
            "4/100\n",
            "5/100\n",
            "6/100\n",
            "7/100\n",
            "8/100\n",
            "9/100\n",
            "10/100\n",
            "11/100\n",
            "12/100\n",
            "13/100\n",
            "14/100\n",
            "15/100\n",
            "16/100\n",
            "17/100\n",
            "18/100\n",
            "19/100\n",
            "20/100\n",
            "21/100\n",
            "22/100\n",
            "23/100\n",
            "24/100\n",
            "25/100\n",
            "26/100\n",
            "27/100\n",
            "28/100\n",
            "29/100\n",
            "30/100\n",
            "31/100\n",
            "32/100\n",
            "33/100\n",
            "34/100\n",
            "35/100\n",
            "36/100\n",
            "37/100\n",
            "38/100\n",
            "39/100\n",
            "40/100\n",
            "41/100\n",
            "42/100\n",
            "43/100\n",
            "44/100\n",
            "45/100\n",
            "46/100\n",
            "47/100\n",
            "48/100\n",
            "49/100\n",
            "50/100\n",
            "51/100\n",
            "52/100\n",
            "53/100\n",
            "54/100\n",
            "55/100\n",
            "56/100\n",
            "57/100\n",
            "58/100\n",
            "59/100\n",
            "60/100\n",
            "61/100\n",
            "62/100\n",
            "63/100\n",
            "64/100\n",
            "65/100\n",
            "66/100\n",
            "67/100\n",
            "68/100\n",
            "69/100\n",
            "70/100\n",
            "71/100\n",
            "72/100\n",
            "73/100\n",
            "74/100\n",
            "75/100\n",
            "76/100\n",
            "77/100\n",
            "78/100\n",
            "79/100\n",
            "80/100\n",
            "81/100\n",
            "82/100\n",
            "83/100\n",
            "84/100\n",
            "85/100\n",
            "86/100\n",
            "87/100\n",
            "88/100\n",
            "89/100\n",
            "90/100\n",
            "91/100\n",
            "92/100\n",
            "93/100\n",
            "94/100\n",
            "95/100\n",
            "96/100\n",
            "97/100\n",
            "98/100\n",
            "99/100\n",
            "100/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(walks, size=128, window=10, min_count=0, sg=1, workers=8, iter=10)\n",
        "wv = model.wv"
      ],
      "metadata": {
        "id": "nfP2XggEpqrV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sims = []\n",
        "for test_edge in test_edges:\n",
        "  src, dst, _ = test_edge\n",
        "  src_vec, dst_vec = wv[src], wv[dst]\n",
        "  cos_sim = src_vec.dot(dst_vec) / np.linalg.norm(src_vec) / np.linalg.norm(dst_vec)\n",
        "  cos_sims.append(cos_sim)\n",
        "cos_sims = np.array(cos_sims)\n",
        "print(\"The prediction accuracy is {}\".format((cos_sims >= 0.5).sum() / len(cos_sims)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hstHsxIsrsyC",
        "outputId": "10ddc0bc-5222-4178-efbe-095317cd0773"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction accuracy is 0.0975609756097561\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "homework_4_2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}